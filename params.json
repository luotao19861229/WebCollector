{"name":"Webcollector","tagline":"A java crawler for infomation collection.","body":"WebCollector\r\n============\r\n\r\nWebCollector is an open source Java crawler which provides some simple interfaces for crawling the Web。You can setup a multi-threaded web crawler in 5 minutes!\r\n\r\n###DEMO\r\nThis DEMO extracts all the questions asked  at [http://www.zhihu.com/](http://www.zhihu.com/) .\r\nYou need to create a crawler class that extends BreadthCrawler.\r\n\r\n\r\n    public class ZhihuCrawler extends BreadthCrawler{\r\n \r\n        /**\r\n         * This function is called when a page is fetched and\r\n         * ready to be processed by your program.       \r\n        */\r\n        @Override\r\n        public void visit(Page page) {\r\n            String question_regex=\"^http://www.zhihu.com/question/[0-9]+\";         \r\n            if(Pattern.matches(question_regex, page.getUrl())){              \r\n                System.out.println(\"processing \"+page.getUrl());\r\n\r\n                /*extract title of the page*/\r\n                String title=page.getDoc().title();\r\n                System.out.println(title);\r\n\r\n                /*extract the content of question*/\r\n                String question=page.getDoc().select(\"div[id=zh-question-detail]\").text();\r\n                System.out.println(question);\r\n             \r\n            }\r\n        }\r\n \r\n        /**\r\n         * start crawling\r\n        */\r\n        public static void main(String[] args) throws IOException{  \r\n            ZhihuCrawler crawler=new ZhihuCrawler();\r\n            crawler.addSeed(\"http://www.zhihu.com/question/21003086\");\r\n            crawler.addRegex(\"http://www.zhihu.com/.*\");\r\n            /*start the crawler with depth=5*/\r\n            crawler.start(5);  \r\n        }\r\n \r\n   \r\n    }\r\n\r\nAs can be seen in the above code,there are one function that should be overridden:\r\n+ __visit(Page page):__ This function is called after the content of a URL is downloaded successfully.You can easily get the url,text of the downloaded page.If the Content-Type of the downloaded page is text/html,you could also get the document and html of the page.The document is a dom tree parsed by JSOUP.The html is a String decoded by detected charset.Page is an instance of cn.edu.hfut.dmic.webcollector.model.Page\r\n * page.getUrl() returns the url of the downloaded page\r\n * page.getContent() returns the origin data of the page\r\n * page.getDoc() returns an instance of org.jsoup.nodes.Document\r\n * page.getResponse() returns the http response of the page\r\n * page.getFetchTime() returns the time this page be fetched at  generated by System.currentTimeMillis()\r\n\r\n\r\n__中文教程:__ [https://github.com/CrawlScript/WebCollector/blob/master/README.zh-cn.md](https://github.com/CrawlScript/WebCollector/blob/master/README.zh-cn.md)\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}